## 移动研究院

### 1、BERT和GPT的区别？它们主要用于什么任务？
Bert和GPT都是深度学习中的NLP模型，它们都使用了Transformer的结构。区别主要在于Bert是encoder部分，是一种自编码模型。而GPT是一种自回归模型。

Bert在训练的时候会随机屏蔽（mask）一些词，然后根据上下文的信息来预测被屏蔽的词。这样模型可以学习到整个文本的语义信息，适合于下游的任务，如：问答，文本分类、实体识别等。

GPT的自回归模型，在训练的时候会利用前面的词来预测下一个词，这样模型学习到文本的生成规律，适合于文本生成、宅哟啊、对话等任务。

### 2、梯度消失的原因？有什么方法可以缓解？
梯度消失的原因是因为在梯度反向传播的过程中，梯度函数有一个累乘项，如果这个累乘项的值小于1，随着层数的增加，梯度会越来越小，最终接近于0.导致网络的权重无法有效的更新。

解决方式：

（1）使用激活函数，如ReLU系列，避免出现过多小于1的值

（2）增加BN层。使输入分布稳定，避免梯度值过大过小

（3）使用残差链接，使梯度直接从后面的层传递到前面，减少累乘

（4）使用门控循环单元（GRU），LSTM，循环神经网络中保持长期的信息流动

### 3、正则化的作用？L1正则化和L2正则化的区别？
正则化是在损失函数中加入一个惩罚函数，用来限制模型的复杂度，防止过拟合。

L1正则化是模型参数的绝对值作为惩罚项，使参数更稀疏，即许多参数为0，从而达到特征选择的效果

L2正则化是模型参数的平方和作为惩罚项，它可以使模型的参数变得平滑，即许多参数的值接近0，从而减少模型的波动


### 4、transformer轻量化的方式？如何减轻其计算复杂度？
Transformer轻量化的方式有以下几种：

在模块级别可以使用低质分解、稀疏注意力、局部注意力等方法来减少注意力机制的计算量

在模型级别，可以使用深度可分卷机、深度可分自注意力、Lite Transformer等方方法来减少模型的参数和计算复杂度

在训练的级别，可以使用知识蒸馏、量化、剪纸等方法，来压缩模型大小，提高模型效率


### 5、GBDT、XGBoost、随机森林的区别？

GBDT，XGBoost和随机森林的区别主要有以下几点：

GBDT和XGBoost都是基于提升树（Boosting Tree）的算法，而随机森林是基于装袋树（Bagging Tree）的算法。

boost是一种串行的结构，每一棵树都是基于前面所有树的残差学习，而装袋树是一种并行的结构，每一棵树都是独立地从样本中抽取数据进行训练。

GBDT和XGBoost都是基于梯度提升（Gradient Boosting）的方法，而随机森林是基于自助法（Bootstrap）的方法。梯度提升是一种利用损失函数的负梯度作为残差的近似值，来优化模型的方法，而自助法是一种有放回地从样本中抽取数据的方法。

XGBoost在GBDT的基础上进行了改进和优化，主要有以下几个方面：

        XGBoost在目标函数中加入了正则项，用于控制模型的复杂度，防止过拟合；
  
        XGBoost先从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝，这样不容易陷入局部最优解；
  
        XGBoost支持并行计算，可以利用多核CPU和GPU加速训练过程；
   
        XGBoost支持自定义损失函数和评估指标，可以适应不同的问题场景；
  
        XGBoost支持缺失值处理，可以自动学习缺失值的最优分割点。

### 6、SVM多分类的方式？
SVM实现多分类的方式主要有两种：

一对一（One-vs-One）：对于K个类别，构造K(K-1)/2个二类分类器，每个分类器只对一对类别进行划分。最后根据投票或者其他规则确定最终的类别。

一对多（One-vs-Rest）：对于K个类别，构造K个二类分类器，每个分类器只对一个类别和其他所有类别进行划分。最后根据置信度或者其他规则确定最终的类别。

### 7、怎么判断一个函数是凸函数？
一阶导数是增函数，二介导数>=0

### 残差连接的作用是什么？

残差链接可以降低模型复杂度以减少过拟合：残差链接可以使模型更容易学到简单的映射，而不是在后面的层中学习复杂的非线性变换。

残差链接可以缓解梯度消失或爆炸的问题：残差链接可以使梯度更容易地从后面的层传递到前面的层，从而保持梯度的稳定性。

残差链接可以提高模型的表达能力：残差链接可以使模型更容易地捕捉到数据中的细微变化，从而提高模型的泛化能力。

