## 移动研究院

### 1、BERT和GPT的区别？它们主要用于什么任务？
Bert和GPT都是深度学习中的NLP模型，它们都使用了Transformer的结构。区别主要在于Bert是encoder部分，是一种自编码模型。而GPT是一种自回归模型。

Bert在训练的时候会随机屏蔽（mask）一些词，然后根据上下文的信息来预测被屏蔽的词。这样模型可以学习到整个文本的语义信息，适合于下游的任务，如：问答，文本分类、实体识别等。

GPT的自回归模型，在训练的时候会利用前面的词来预测下一个词，这样模型学习到文本的生成规律，适合于文本生成、宅哟啊、对话等任务。

### 2、梯度消失的原因？有什么方法可以缓解？
梯度消失的原因是因为在梯度反向传播的过程中，梯度函数有一个累乘项，如果这个累乘项的值小于1，随着层数的增加，梯度会越来越小，最终接近于0.导致网络的权重无法有效的更新。

解决方式：
（1）使用激活函数，如ReLU系列，避免出现过多戏哦啊雨1的值
（2）增加BN层。使输入分布稳定，避免梯度值过大过小
（3）使用残差链接，使梯度直接从后面的层传递到前面，减少累乘
（4）使用门控循环单元（GRU），LSTM，循环神经网络中保持长期的信息流动

### 3、正则化的作用？L1正则化和L2正则化的区别？

### 4、transformer轻量化的方式？如何减轻其计算复杂度？

### 5、GBDT、XGBoost、随机森林的区别？

### 6、SVM多分类的方式？

### 7、怎么判断一个函数是凸函数？

